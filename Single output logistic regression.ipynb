{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(X,Y):\n",
    "    '''\n",
    "    Input:\n",
    "        X:Input dataset\n",
    "        Y:Output Label\n",
    "        \n",
    "    Output:\n",
    "        (W,b): A tuple containing zero initialized weights and bias\n",
    "    '''\n",
    "    \n",
    "    nx=X.shape[0]\n",
    "    W=np.zeros((1,nx))\n",
    "    b=0\n",
    "    return (W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11783332 -0.01019626  1.51513948 -1.30678613 -1.54888668]\n",
      " [-1.32992628  0.83867087 -0.12718312 -0.30409439  0.05737848]\n",
      " [ 0.8584816  -0.41655815 -1.08443734  0.90168388  0.35150646]\n",
      " [-0.18121718  1.21221386  0.50375548  0.39321191 -0.83239343]\n",
      " [ 0.78728078  1.73619699 -1.15818032 -1.16355088  1.61002012]\n",
      " [ 0.54140064 -1.47942387  0.0410408  -1.20398055  1.46760332]\n",
      " [ 0.45870554  0.31148382  1.07681513  0.15825904 -1.99596624]\n",
      " [ 1.88045671 -0.72316082 -0.05926425  2.02857442  1.15763139]\n",
      " [ 0.26597638 -1.34557769 -0.48739755  0.0935297  -0.32621853]\n",
      " [ 1.86923613 -1.04821609  0.03853083  0.38342816 -0.03627887]] (10, 5)\n",
      "[[0 1 0 1 1]] (1, 5)\n"
     ]
    }
   ],
   "source": [
    "X=np.random.randn(10,5)\n",
    "Y=np.array([[0,1,0,1,1]])\n",
    "print(X,X.shape)\n",
    "print(Y,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(initialize_weights(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input:\n",
    "        Z:Array\n",
    "    Output:\n",
    "        sigmoid value on each element of Z\n",
    "    '''\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(parameters,X,Y):\n",
    "    '''\n",
    "    Input:\n",
    "        parameters:A tuple containing weight and bias matrix\n",
    "        X:Input Data\n",
    "        Y:Output Labels\n",
    "    Output:\n",
    "        gradient:dictionary containing dw and db which is gradient of loss w.r.t w and b respectively\n",
    "        Loss:Loss between predicted value and actual value\n",
    "    '''\n",
    "    m=X.shape[1]\n",
    "    w=parameters[0]\n",
    "    b=parameters[1]\n",
    "    z=np.dot(w,X)+b\n",
    "    a=sigmoid(z)\n",
    "    \n",
    "    Loss=-1/m*np.sum((Y*np.log(a)+(1-Y)*np.log(1-a)))\n",
    "    dz=a-Y\n",
    "    dw=1/m*np.dot(dz,X.T)\n",
    "    db=1/m*np.sum(dz)\n",
    "    \n",
    "    gradient={'dw':dw,\n",
    "             'db':db}\n",
    "    \n",
    "    return gradient,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dw': array([[0.99845601, 2.39507239]]), 'db': 0.001455578136784208}\n",
      "5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1., 2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grad,cost=forward_propagate((w,b),X,Y)\n",
    "print(grad)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(alpha,parameters,X,Y,num_iterations,printValue=False):\n",
    "    '''\n",
    "    Input:\n",
    "        alpha:learning rate for our modek\n",
    "        parameters:A tuple containing weight and bias\n",
    "        X:Input Data\n",
    "        Y:Output Labels\n",
    "        num_iterations: Number of iterations upto which model is run\n",
    "        printValue:parameter whether or not to print the value of cost after every 100 iterations (default:False)\n",
    "    Output:\n",
    "        costList: List Of costs after every 100 iterations\n",
    "        param: Tuple of parameters(weight and bias) after applying gradient descent\n",
    "        grads: dictionary containing the value of dw and db after applying gradient descent\n",
    "    '''\n",
    "    w=parameters[0]\n",
    "    b=parameters[1]\n",
    "    costList=[]\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        gradient,cost=forward_propagate((w,b),X,Y)\n",
    "        dw=gradient['dw']\n",
    "        db=gradient['db']\n",
    "        w=w-alpha*dw\n",
    "        b=b-alpha*db\n",
    "        if(i%100==0):\n",
    "            if(printValue):\n",
    "                print(cost)\n",
    "            costList.append(cost)\n",
    "    param=(w,b)\n",
    "    grads={'dw':dw,\n",
    "          'db':db}\n",
    "    return costList,param,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters,X):\n",
    "    '''\n",
    "    Input:\n",
    "        parameters:A tuple containing weights and bias\n",
    "        X:Input Data\n",
    "    Output:\n",
    "        predicted: Predicted values on our data\n",
    "    '''\n",
    "    w=parameters[0]\n",
    "    b=parameters[1]\n",
    "    z=np.dot(w,X)+b\n",
    "    a=sigmoid(z)\n",
    "    \n",
    "    predicted=np.around(a)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,alpha,num_iterations):\n",
    "    '''\n",
    "    Input:\n",
    "        X:Input Data\n",
    "        Y:Output Data\n",
    "        alpha:Learning rate\n",
    "        num_iterations:number of iterations for which gradient descent is run\n",
    "    Output:\n",
    "        parameter: parameter obtained after applying gradient descent\n",
    "        cost:List of cost containing values of cost after every 100 iterations\n",
    "        accuracy: Accuracy obtained on Input Data\n",
    "    '''\n",
    "    parameters=initialize_weights(X,Y)\n",
    "    cost,parameter,gradient=optimize(alpha,parameters,X,Y,num_iterations)\n",
    "    Y_predicted=predict(parameter,X)\n",
    "    m=Y.shape[1]\n",
    "    count=0\n",
    "    for i in range(m):\n",
    "        if(Y_predicted[0][i]==Y[0][i]):\n",
    "            count+=1        \n",
    "    accuracy=count/m*100\n",
    "    return parameter,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
